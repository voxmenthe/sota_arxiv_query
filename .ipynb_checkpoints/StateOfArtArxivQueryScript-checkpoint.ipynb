{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install -y libenchant1c2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyenchant nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* Neaten up the \"does not include\" - identify bottleneck point\n",
    "* Make the enchant dependency optional\n",
    "\n",
    "#### Implement the following:\n",
    "* Downloading results to file\n",
    "* Boolean search terms\n",
    "* Filter by year and/or by date range\n",
    "* One abbreviated call\n",
    "* Specify partial word match or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import urllib.error\n",
    "import urllib.request\n",
    "import enchant\n",
    "from nltk.tokenize import word_tokenize\n",
    "from six.moves.html_parser import HTMLParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Term Ideas\n",
    "#### image retrieval, bag of features, visual similarity, (content based) similarity search, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple keywords can be entered, separated by phrase\n",
    "search_term = \"visual similar\"\n",
    "does_not_include = \"phonetic\" # None\n",
    "num_results = 100\n",
    "RESULTS_PER_PAGE = 200\n",
    "\n",
    "# set SOTA flag to True to filter for only \"state of the art\" results\n",
    "SOTA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HTMLParser()\n",
    "\n",
    "AUTHOR_TAG = '<a href=\"/search/?searchtype=author'\n",
    "TITLE_TAG = '<p class=\"title is-5 mathjax\">'\n",
    "ABSTRACT_TAG = '<span class=\"abstract-full has-text-grey-dark mathjax\"'\n",
    "DATE_TAG = '<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span>'\n",
    "\n",
    "\n",
    "def get_authors(lines, i):\n",
    "    authors = []\n",
    "    while True:\n",
    "        if not lines[i].startswith(AUTHOR_TAG):\n",
    "            break\n",
    "        idx = lines[i].find('>')\n",
    "        if lines[i].endswith(','):\n",
    "            authors.append(lines[i][idx + 1: -5])\n",
    "        else:\n",
    "            authors.append(lines[i][idx + 1: -4])\n",
    "        i += 1\n",
    "    return authors, i\n",
    "\n",
    "\n",
    "def get_next_result(lines, start):\n",
    "    \"\"\"\n",
    "    Extract paper from the xml file obtained from arxiv search.\n",
    "    \n",
    "    Each paper is a dict that contains:\n",
    "    + 'title': str\n",
    "    + 'pdf_link': str\n",
    "    + 'main_page': str\n",
    "    + 'authors': []\n",
    "    + 'abstract': str\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    idx = lines[start + 3][10:].find('\"')\n",
    "    result['main_page'] = lines[start + 3][9:10 + idx]\n",
    "    idx = lines[start + 4][23:].find('\"')\n",
    "    result['pdf'] = lines[start + 4][22: 23 + idx] + '.pdf'\n",
    "\n",
    "    start += 4\n",
    "\n",
    "    while lines[start].strip() != TITLE_TAG:\n",
    "        start += 1\n",
    "\n",
    "    title = lines[start + 1].strip()\n",
    "    title = title.replace('<span class=\"search-hit mathjax\">', '')\n",
    "    title = title.replace('</span>', '')\n",
    "    result['title'] = title\n",
    "\n",
    "    authors, start = get_authors(lines, start + 5)  # orig: add 8\n",
    "\n",
    "    while not lines[start].strip().startswith(ABSTRACT_TAG):\n",
    "        start += 1\n",
    "    abstract = lines[start + 1]\n",
    "    abstract = abstract.replace('<span class=\"search-hit mathjax\">', '')\n",
    "    abstract = abstract.replace('</span>', '')\n",
    "    result['abstract'] = abstract\n",
    "\n",
    "    result['authors'] = authors\n",
    "\n",
    "    while not lines[start].strip().startswith(DATE_TAG):\n",
    "        start += 1\n",
    "\n",
    "    idx = lines[start].find('</span> ')\n",
    "    end = lines[start][idx:].find(';')\n",
    "\n",
    "    result['date'] = lines[start][idx + 8: idx + end]\n",
    "\n",
    "    return result, start\n",
    "\n",
    "\n",
    "def clean_empty_lines(lines):\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            cleaned.append(line)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def is_float(token):\n",
    "    return re.match(\"^\\d+?\\.\\d+?$\", token) is not None\n",
    "\n",
    "\n",
    "def is_citation_year(tokens, i):\n",
    "    if len(tokens[i]) != 4:\n",
    "        return False\n",
    "    if re.match(r'[12][0-9]{3}', tokens[i]) is None:\n",
    "        return False\n",
    "    if i == 0 or i == len(tokens) - 1:\n",
    "        return False\n",
    "    if (tokens[i - 1] == ',' or tokens[i - 1] == '(') and tokens[i + 1] == ')':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_list_numer(tokens, i, value):\n",
    "    if value < 1 or value > 4:\n",
    "        return False\n",
    "    if i == len(tokens) - 1:\n",
    "        return False\n",
    "\n",
    "    if (i == 0 or tokens[i - 1] in set(['(', '.', ':'])) and tokens[i + 1] == ')':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def has_number(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.endswith('\\\\'):\n",
    "            token = token[:-2]\n",
    "        if token.endswith('x'):  # sometimes people write numbers as 1.7x\n",
    "            token = token[:-1]\n",
    "        if token.startswith('x'):  # sometimes people write numbers as x1.7\n",
    "            token = token[1:]\n",
    "        if token.startswith('$') and token.endswith('$'):\n",
    "            token = token[1:-1]\n",
    "        if is_float(token):\n",
    "            return True\n",
    "        try:\n",
    "            value = int(token)\n",
    "        except:\n",
    "            continue\n",
    "        if (not is_citation_year(tokens, i)) and (not is_list_numer(tokens, i, value)):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def contains_sota(sent):\n",
    "    return 'state-of-the-art' in sent or 'state of the art' in sent or 'SOTA' in sent\n",
    "\n",
    "\n",
    "def extract_line(abstract, keyword, does_not_include, limit):\n",
    "    lines = []\n",
    "    numbered_lines = []\n",
    "    kw_mentioned = False\n",
    "    abstract = abstract.replace(\"et. al\", \"et al.\")\n",
    "    sentences = abstract.split('. ')\n",
    "    kw_sentences = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if keyword in sent.lower() and does_not_include not in sent.lower():\n",
    "            kw_mentioned = True\n",
    "            if has_number(sent):\n",
    "                numbered_lines.append(sent)\n",
    "            elif contains_sota(sent):\n",
    "                numbered_lines.append(sent)\n",
    "            else:\n",
    "                kw_sentences.append(sent)\n",
    "                lines.append(sent)\n",
    "            continue\n",
    "\n",
    "        if kw_mentioned and has_number(sent):\n",
    "            if not numbered_lines:\n",
    "                numbered_lines.append(kw_sentences[-1])\n",
    "            numbered_lines.append(sent)\n",
    "        if SOTA and kw_mentioned and contains_sota(sent):\n",
    "            lines.append(sent)\n",
    "        elif kw_mentioned:\n",
    "            lines.append(sent)\n",
    "\n",
    "    if len(numbered_lines) > 0:\n",
    "        return '. '.join(numbered_lines), True\n",
    "    return '. '.join(lines[-2:]), False\n",
    "\n",
    "\n",
    "def get_report(paper, keyword, does_not_include):\n",
    "    if keyword in paper['abstract'].lower() and does_not_include not in paper['abstract'].lower():\n",
    "        title = html.unescape(paper['title'])\n",
    "        headline = '{} ({} - {})\\n'.format(title, paper['authors'][0], paper['date'])\n",
    "        abstract = html.unescape(paper['abstract'])\n",
    "        extract, has_number = extract_line(abstract, keyword, does_not_include, 280 - len(headline))\n",
    "        if extract:\n",
    "            report = headline + extract + '\\nLink: {}'.format(paper['main_page'])\n",
    "            return report, has_number\n",
    "    return '', False\n",
    "\n",
    "\n",
    "def txt2reports(txt, keyword, does_not_include, num_to_show):\n",
    "    found = False\n",
    "    txt = ''.join(chr(c) for c in txt)\n",
    "    lines = txt.split('\\n')\n",
    "    lines = clean_empty_lines(lines)\n",
    "    unshown = []\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        if num_to_show <= 0:\n",
    "            return unshown, num_to_show, found\n",
    "\n",
    "        line = lines[i].strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if line == '<li class=\"arxiv-result\">':\n",
    "            found = True\n",
    "            paper, i = get_next_result(lines, i)\n",
    "            report, has_number = get_report(paper, keyword, does_not_include)\n",
    "\n",
    "            if has_number:\n",
    "                print(report)\n",
    "                print('====================================================')\n",
    "                num_to_show -= 1\n",
    "            elif report:\n",
    "                unshown.append(report)\n",
    "        if line == '</ol>':\n",
    "            break\n",
    "    return unshown, num_to_show, found\n",
    "\n",
    "\n",
    "def get_papers(keyword, does_not_include, num_results=5, per_page=200):\n",
    "    \"\"\"\n",
    "    If keyword is an English word, then search in CS category only to avoid papers from other categories, resulted from the ambiguity\n",
    "    \"\"\"\n",
    "\n",
    "    if keyword in set(['GAN', 'bpc']):\n",
    "        query_temp = 'https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term={}&terms-0-field=all&classification-computer_science=y&classification-physics_archives=all&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size={}&order=-announced_date_first&start={}'\n",
    "        keyword = keyword.lower()\n",
    "    else:\n",
    "        keyword = keyword.lower()\n",
    "        d = enchant.Dict('en_US')\n",
    "        if d.check(keyword):\n",
    "            query_temp = 'https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term={}&terms-0-field=all&classification-computer_science=y&classification-physics_archives=all&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size={}&order=-announced_date_first&start={}'\n",
    "        else:\n",
    "            query_temp = 'https://arxiv.org/search/?searchtype=all&query={}&abstracts=show&size={}&order=-announced_date_first&start={}'\n",
    "    keyword_q = keyword.replace(' ', '+')\n",
    "    page = 0\n",
    "    num_to_show = num_results\n",
    "    all_unshown = []\n",
    "\n",
    "    while num_to_show > 0:\n",
    "        query = query_temp.format(keyword_q, str(per_page), str(per_page * page))\n",
    "\n",
    "        req = urllib.request.Request(query)\n",
    "        try:\n",
    "            response = urllib.request.urlopen(req)\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print('Error {}: problem accessing the server'.format(e.code))\n",
    "            return\n",
    "\n",
    "        txt = response.read()\n",
    "        unshown, num_to_show, found = txt2reports(txt, keyword, does_not_include, num_to_show)\n",
    "        if not found and not all_unshown and num_to_show == num_results:\n",
    "            print('Sorry, we were unable to find any abstract with the word {}'.format(keyword))\n",
    "            return\n",
    "\n",
    "        if num_to_show < num_results / 2 or not found:\n",
    "            for report in all_unshown[:num_to_show]:\n",
    "                print(report)\n",
    "                print('====================================================')\n",
    "            if not found:\n",
    "                return\n",
    "            num_to_show -= len(all_unshown)\n",
    "        else:\n",
    "            all_unshown.extend(unshown)\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(keyword=search_term,does_not_include=does_not_include, num_results=num_results, per_page=RESULTS_PER_PAGE):\n",
    "    get_papers(keyword, does_not_include, num_results, per_page=per_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developmental Bayesian Optimization of Black-Box with Visual Similarity-Based Transfer Learning (Amaury Depierre - 3 October, 2018)\n",
      "In simulation, we demonstrate the benefit of the transfer learning based on visual similarity, as opposed to an amnesic learning (i.e. Moreover, with the real robot, we show that the method consistently outperforms the manual optimization from an expert with less than 2 hours of training time to achieve more than 88% of success.\n",
      "Link: https://arxiv.org/abs/1809.10141\n",
      "====================================================\n",
      "Class2Str: End to End Latent Hierarchy Learning (Soham Saha - 20 August, 2018)\n",
      "However, classes have visual similarities and often form a hierarchy. Compared to the previous work of HDCNN, which also learns a 2 level hierarchy, we are able to learn a hierarchy at an arbitrary number of levels as well as obtain an accuracy improvement on the Imagenet classification task over them\n",
      "Link: https://arxiv.org/abs/1808.06675\n",
      "====================================================\n",
      "Fast Video Shot Transition Localization with Deep Structured Models (Shitao Tang - 13 August, 2018)\n",
      "However, localization of gradual transitions are still under-explored due to the high visual similarity between adjacent frames. With one TITAN GPU, the proposed method can achieve a 30\\(\\times\\) real-time speed. In order to train a high-performance shot transition detector, we contribute a new database ClipShots, which contains 128636 cut transitions and 38120 gradual transitions from 4039 online videos\n",
      "Link: https://arxiv.org/abs/1808.04234\n",
      "====================================================\n",
      "Feature learning based on visual similarity triplets in medical image analysis: A case study of emphysema in chest CT scans (Silas Nyboe Ãrting - 19 June, 2018)\n",
      "We derive visual similarity triplets from visually assessed emphysema extent and learn a low dimensional embedding using CNNs. We evaluate the networks on 973 images, and show that the CNNs can learn disease relevant feature representations from derived similarity triplets\n",
      "Link: https://arxiv.org/abs/1806.07131\n",
      "====================================================\n",
      "Detecting Homoglyph Attacks with a Siamese Neural Network (Jonathan Woodbridge - 24 May, 2018)\n",
      "Rather than leveraging similarity based on character swaps and deletions, this technique uses a learned metric on strings rendered as images: a CNN learns features that are optimized to detect visual similarity of the rendered strings. This technique shows a considerable 13% to 45% improvement over baseline techniques in terms of area under the receiver operating characteristic curve (ROC AUC)\n",
      "Link: https://arxiv.org/abs/1805.09738\n",
      "====================================================\n",
      "Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination (Zhirong Wu - 4 May, 2018)\n",
      "Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.\n",
      "Link: https://arxiv.org/abs/1805.01978\n",
      "====================================================\n",
      "A Neural Embeddings Approach for Detecting Mobile Counterfeit Apps (Jathushan Rajasegaran - 25 April, 2018)\n",
      "Under a conservative assumption, we were able to find 139 apps that contain malware in a set of 6,880 apps that showed high visual similarity to one of the top-1,000 apps in Google Play Store.\n",
      "Link: https://arxiv.org/abs/1804.09882\n",
      "====================================================\n",
      "Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised Object Detection (Yuxing Tang - 13 March, 2018)\n",
      "We found strong evidence that visual similarity and semantic relatedness are complementary for the task, and when combined notably improve detection, achieving state-of-the-art detection performance in a semi-supervised setting.\n",
      "Link: https://arxiv.org/abs/1801.03145\n",
      "====================================================\n",
      "Multi-class Semantic Segmentation of Skin Lesions via Fully Convolutional Networks (Manu Goyal - 28 November, 2017)\n",
      "There are three main types of skin lesions in common that are benign nevi, melanoma, and seborrhoeic keratoses which have huge intra-class variations in terms of color, size, place and appearance for each class and high inter-class visual similarities in dermoscopic images. We propose a multiclass semantic segmentation for these three classes from publicly available ISBI-2017 challenge dataset which consists of 2750 dermoscopic images. The results showed that the two-tier level transfer learning FCN-8s achieved the overall best result with Dice score of 0.785 in a benign category, 0.653 in melanoma segmentation, and 0.557 in seborrhoeic keratoses.\n",
      "Link: https://arxiv.org/abs/1711.10449\n",
      "====================================================\n",
      "Scalable Annotation of Fine-Grained Categories Without Experts (Timnit Gebru - 7 September, 2017)\n",
      "In animals, there is a direct link between taxonomy and visual similarity: e.g. a 2011 Ford F-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very different from a 2011 Ford F-150 Supercrew-SVT\n",
      "Link: https://arxiv.org/abs/1709.02482\n",
      "====================================================\n",
      "Learning Non-Metric Visual Similarity for Image Retrieval (Noa Garcia - 5 September, 2017)\n",
      "Traditionally, content-based image retrieval systems rely on two fundamental tasks: 1) computing meaningful image representations from pixels and 2) measuring accurate visual similarity between those representations\n",
      "Link: https://arxiv.org/abs/1709.01353\n",
      "====================================================\n",
      "Instance Flow Based Online Multiple Object Tracking (Sebastian Bullinger - 15 May, 2017)\n",
      "We define an affinity matrix between instances of subsequent frames which reflects locality and visual similarity. We evaluate different configurations of our algorithm using the MOT 2D 2015 train dataset. In addition, we provide results of our approach on the MOT 2D 2015 test set for comparison with previous works. We achieve a MOTA score of 32.1.\n",
      "Link: https://arxiv.org/abs/1703.01289\n",
      "====================================================\n",
      "Skin Lesion Analysis Towards Melanoma Detection Using Deep Learning Network (Yuexiang Li - 22 November, 2017)\n",
      "low contrast between lesions and skin, visual similarity between melanoma and non-melanoma lesions, etc. In this paper, we proposed two deep learning methods to address all the three tasks announced in ISIC 2017, i.e. lesion segmentation (task 1), lesion dermoscopic feature extraction (task 2) and lesion classification (task 3). The proposed deep learning frameworks were evaluated on the ISIC 2017 testing set. 0.718 for task 1, 0.833 for task 2 and 0.823 for task 3 were achieved.\n",
      "Link: https://arxiv.org/abs/1703.00577\n",
      "====================================================\n",
      "Learning to Hash-tag Videos with Tag2Vec (Aditya Singh - 13 December, 2016)\n",
      "Traditional data-driven approaches for tag enrichment and recommendation use direct visual similarity for label transfer and propagation. We first employ a natural language processing (NLP) technique, skip-gram models with neural network training to learn a low-dimensional vector representation of hash-tags (Tag2Vec) using a corpus of 10 million hash-tags. We learn this embedding for 29 categories of short video clips with hash-tags\n",
      "Link: https://arxiv.org/abs/1612.04061\n",
      "====================================================\n",
      "Heritability maps of human face morphology through large-scale automated three-dimensional phenotyping (Dimosthenis Tsagkrasoulis - 28 January, 2017)\n",
      "The human face is a complex trait under strong genetic control, as evidenced by the striking visual similarity between twins. High-resolution, three-dimensional facial models have been acquired on a cohort of $952$ twins recruited from the TwinsUK registry, and processed through a novel landmarking workflow, GESSA (Geodesic Ensemble Surface Sampling Algorithm)\n",
      "Link: https://arxiv.org/abs/1608.08199\n",
      "====================================================\n",
      "Connectionist Temporal Modeling for Weakly Supervised Action Labeling (De-An Huang - 28 July, 2016)\n",
      "We address this by introducing the Extended Connectionist Temporal Classification (ECTC) framework to efficiently evaluate all possible alignments via dynamic programming and explicitly enforce their consistency with frame-to-frame visual similarities. With less than 1% of labeled frames per video, our method is able to outperform existing semi-supervised approaches and achieve comparable performance to that of fully supervised approaches.\n",
      "Link: https://arxiv.org/abs/1607.08584\n",
      "====================================================\n",
      "SEMBED: Semantic Embedding of Egocentric Action Videos (Michael Wray - 29 July, 2016)\n",
      "When object interactions are annotated using unbounded choice of verbs, we embrace the wealth and ambiguity of these labels by capturing the semantic relationships as well as the visual similarities over motion and appearance features. We show how SEMBED can interpret a challenging dataset of 1225 freely annotated egocentric videos, outperforming SVM classification by more than 5%.\n",
      "Link: https://arxiv.org/abs/1607.08414\n",
      "====================================================\n",
      "Sequence to sequence learning for unconstrained scene text recognition (Ahmed Mamdouh A. Hassanien - 20 July, 2016)\n",
      "While the CNN gives very good recognition results, it does not model relation between characters, hence gives rise to false positive and false negative cases (confusing characters due to visual similarities like \"g\" and \"9\", or confusing background patches with characters; either removing existing characters or adding non-existing ones) To alleviate these problems we leverage recent developments in LSTM architectures to encode contextual information. We use the ICDAR 13 test set for evaluation and compare the results with the state of the art approaches [11, 18]\n",
      "Link: https://arxiv.org/abs/1607.06125\n",
      "====================================================\n",
      "Visual Congruent Ads for Image Search (Yannis Kalantidis - 21 April, 2016)\n",
      "Our method compares the visual similarity of candidate ads to the image search results and selects the most visually similar ad to be displayed. We conduct an experiment with about 900 users and find that our proposed method provides significant improvement in the users' overall satisfaction with the image search experience, without diminishing the users' ability to see the ad or recall the advertised brand.\n",
      "Link: https://arxiv.org/abs/1604.06481\n",
      "====================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity-based Text Recognition by Deeply Supervised Siamese Network (Ehsan Hosseini-Asl - 4 July, 2016)\n",
      "The Deeply Supervised Siamese network learns visual similarity of texts. We demonstrate that the model reduces the cost of human estimation by $50\\%-85\\%$\n",
      "Link: https://arxiv.org/abs/1511.04397\n",
      "====================================================\n",
      "Unsupervised Learning on Neural Network Outputs: with Application in Zero-shot Learning (Yao Lu - 23 May, 2016)\n",
      "The PCA/ICA embedding of the object classes reveals their visual similarity and the PCA/ICA components can be interpreted as common visual features shared by similar object classes. Our zero-shot learning method achieves the state-of-the-art results on the ImageNet of over 20000 classes.\n",
      "Link: https://arxiv.org/abs/1506.00990\n",
      "====================================================\n",
      "Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network (Junshi Huang - 29 May, 2015)\n",
      "In addition, to further align with the nature of the retrieval problem, we impose a triplet visual similarity constraint for learning to rank across the two sub-networks. The top-20 retrieval accuracy is doubled when using the proposed DARN other than the current popular solution using pre-trained CNN features only (0.570 vs. 0.268).\n",
      "Link: https://arxiv.org/abs/1505.07922\n",
      "====================================================\n",
      "Unsupervised Visual Representation Learning by Context Prediction (Carl Doersch - 16 January, 2016)\n",
      "We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset\n",
      "Link: https://arxiv.org/abs/1505.05192\n",
      "====================================================\n",
      "Are the Variability Properties of the Kepler AGN Light Curves Consistent with a Damped Random Walk? (Vishal P. Kasliwal - 2 June, 2015)\n",
      "We categorize the light curves of all 20 objects based on visual similarities and find that the light curves fall into 5 broad categories. We find that less than half the objects are consistent with a DRW and observe variability on short timescales ($\\sim 2$ h)\n",
      "Link: https://arxiv.org/abs/1505.00360\n",
      "====================================================\n",
      "Self-informed neural network structure learning (David Warde-Farley - 13 April, 2015)\n",
      "Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning of the label space into groups of visually similar entities. We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%.\n",
      "Link: https://arxiv.org/abs/1412.6563\n",
      "====================================================\n",
      "Hybrid Affinity Propagation (Jingdong Wang - 30 July, 2013)\n",
      "The main advantages of our approach lie in 1) that $\\text{H}^\\text{2}\\text{MP}$ exploits visual similarity and in addition the useful information from the associated tags, including the associations relation between images and tags and the relations within tags, and 2) that the summary is both visually and semantically satisfactory\n",
      "Link: https://arxiv.org/abs/1307.7851\n",
      "====================================================\n",
      "Study of the 3D Coronal Magnetic Field of Active Region 11117 Around the Time of a Confined Flare Using a Data-Driven CESE--MHD Model (Chaowei Jiang - 10 September, 2012)\n",
      "The model qualitatively reproduces the basic structures of the 3D magnetic field, as supported by the visual similarity between the field lines and the coronal loops observed by the Atmospheric Imaging Assembly (AIA), which shows that the coronal field can indeed be well characterized by the MHD equilibrium in most time. The total magnetic flux and energy keep increasing slightly in spite of the flare, while the computed magnetic free energy drops during the flare with an amount of $\\sim 10^{30}$ erg, which seems to be adequate to provide the energy budget of the minor C-class confined flare.\n",
      "Link: https://arxiv.org/abs/1209.2207\n",
      "====================================================\n",
      "Predicting Regional Classification of Levantine Ivory Sculptures: A Machine Learning Approach (Amy Rebecca Gansell - 27 June, 2008)\n",
      "Based on the visual similarity of sculptures, individuals within these fields have proposed object assemblages linked to hypothesized regional production centers. We first construct a prediction function using 66 categorical features as inputs and regional style as output. The model assigns regional style group (RSG), with 98 percent prediction accuracy\n",
      "Link: https://arxiv.org/abs/0806.4642\n",
      "====================================================\n",
      "Segmentation, Indexing, and Visualization of Extended Instructional Videos (Alexander Haubold - 16 February, 2003)\n",
      "Given a series of key frames from the video, we generate a condensed view of the data by clustering frames according to media type and visual similarities. We analyze the accuracy of the segmentation tool on 17 instructional videos, each of which is from 75 to 150 minutes in duration (a total of 40 hours); the classification accuracy exceeds 96%.\n",
      "Link: https://arxiv.org/abs/cs/0302023\n",
      "====================================================\n",
      "SVS-JOIN: Efficient Spatial Visual Similarity Join over Multimedia Data (Chengyuan Zhang - 1 October, 2018)\n",
      "To further improve the performance of search, we develop a novel approach called SVS-JOIN$_Q$ which utilizes a quadtree and a global inverted index. Experimental evaluations on real geo-image datasets demonstrate that our solution has a really high performance.\n",
      "Link: https://arxiv.org/abs/1810.00549\n",
      "====================================================\n",
      "Hierarchy-based Image Embeddings for Semantic Image Retrieval (BjÃ¶rn Barz - 26 September, 2018)\n",
      "We introduce a deterministic algorithm for computing the class centroids directly based on prior world-knowledge encoded in a hierarchy of classes such as WordNet. Experiments on CIFAR-100 and ImageNet show that our learned semantic image embeddings improve the semantic consistency of image retrieval results by a large margin.\n",
      "Link: https://arxiv.org/abs/1809.09924\n",
      "====================================================\n",
      "Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance (Siyu Huang - 22 August, 2018)\n",
      "Our proposed pooling modules do not introduce extra parameters into model and can easily take place of the vanilla pooling layer in implementation. In empirical study on two benchmark crowd counting datasets, the stacked pooling beats the vanilla pooling layer in most cases.\n",
      "Link: https://arxiv.org/abs/1808.07456\n",
      "====================================================\n",
      "Image-based remapping of spatially-varying material appearance (Alejandro Sztrajman - 20 August, 2018)\n",
      "Therefore, we further present a parametric regression scheme that approximates the shape of the transformation function and generates a reduced representation which evaluates instantly and without further interaction with the renderer. We present respective visual comparisons of the remapped SVBRDF models for commonly used renderers and shading models, and show that our approach is able to extrapolate transformed BRDF parameters better than other complex regression schemes.\n",
      "Link: https://arxiv.org/abs/1808.06715\n",
      "====================================================\n",
      "Person Re-identification with Deep Similarity-Guided Graph Neural Network (Yantao Shen - 26 July, 2018)\n",
      "Different from conventional GNN approaches, SGGNN learns the edge weights with rich labels of gallery instance pairs directly, which provides relation fusion more precise information. The effectiveness of our proposed method is validated on three public person re-identification datasets.\n",
      "Link: https://arxiv.org/abs/1807.09975\n",
      "====================================================\n",
      "Computer Analysis of Architecture Using Automatic Image Understanding (Fan Wei - 12 July, 2018)\n",
      "This experiment provides a new paradigm for studying architecture, based on a quantitative approach that can enhance the traditional manual observation and analysis. The source code used for the analysis is open and publicly available.\n",
      "Link: https://arxiv.org/abs/1807.04892\n",
      "====================================================\n",
      "Learning a Saliency Evaluation Metric Using Crowdsourced Perceptual Judgments (Changqun Xia - 26 June, 2018)\n",
      "Experimental results validate that the learned metric can be generalized to the comparisons of saliency maps from new images, new datasets, new models and synthetic data. Due to the effectiveness of the learned metric, it also can be used to facilitate the development of new models for fixation prediction.\n",
      "Link: https://arxiv.org/abs/1806.10257\n",
      "====================================================\n",
      "Collaborative Human-AI (CHAI): Evidence-Based Interpretable Melanoma Classification in Dermoscopic Images (Noel C. F. Codella - 1 August, 2018)\n",
      "Results are improved over baselines trained on disease labels alone, as well as standard multiclass loss. Quantitative relevance of results, according to non-expert similarity, as well as localized image regions, are also significantly improved.\n",
      "Link: https://arxiv.org/abs/1805.12234\n",
      "====================================================\n",
      "BigDL: A Distributed Deep Learning Framework for Big Data (Jason Dai - 24 June, 2018)\n",
      "Since its initial open source release, BigDL users have built many analytics and deep learning applications (e.g., object detection, sequence-to-sequence generation, visual similarity, neural recommendations, fraud detection, etc.) on Spark.\n",
      "Link: https://arxiv.org/abs/1804.05839\n",
      "====================================================\n",
      "Efficient and Deep Person Re-Identification using Multi-Level Similarity (Yiluan Guo - 1 April, 2018)\n",
      "Our work is the first to propose to compute visual similarities at low, middle and high levels for ReID. With extensive experiments and analysis, we demonstrate that our system, compact yet effective, can achieve competitive results with much smaller model size and computational complexity.\n",
      "Link: https://arxiv.org/abs/1803.11353\n",
      "====================================================\n",
      "A Framework for Video-Driven Crowd Synthesis (Jordan Stadler - 13 March, 2018)\n",
      "We also propose a new metric for comparing the \"visual similarity\" between the synthesized crowd and exemplar crowd. We demonstrate the proposed approach on crowd videos collected under different settings.\n",
      "Link: https://arxiv.org/abs/1803.04969\n",
      "====================================================\n",
      "Deep Unsupervised Learning of Visual Similarities (Artsiom Sanakoyeu - 21 February, 2018)\n",
      "The CNN then consolidates transitivity relations within and between groups and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.\n",
      "Link: https://arxiv.org/abs/1802.08562\n",
      "====================================================\n",
      "Semantic Image Retrieval via Active Grounding of Visual Situations (Max H. Quinn - 31 October, 2017)\n",
      "Such scores can be used to rank images in a collection as part of a retrieval system. In the preliminary study described here, we demonstrate the promise of this system by comparing Situate's performance with that of two baseline methods, as well as with a related semantic image-retrieval system based on \"scene graphs.\"\n",
      "Link: https://arxiv.org/abs/1711.00088\n",
      "====================================================\n",
      "Addressing Challenging Place Recognition Tasks using Generative Adversarial Networks (Yasir Latif - 26 February, 2018)\n",
      "Experiments show that learned features correspond to visual similarity and can be effectively used for place recognition across seasons.\n",
      "Link: https://arxiv.org/abs/1709.08810\n",
      "====================================================\n",
      "Cross-Media Similarity Evaluation for Web Image Retrieval in the Wild (Jianfeng Dong - 6 January, 2018)\n",
      "Consequently, computing cross-media similarity between the test query and a given image boils down to comparing the visual similarity between the given image and the selected images. Image retrieval experiments on the challenging Clickture dataset show that the proposed text2image compares favorably to recent deep learning based alternatives.\n",
      "Link: https://arxiv.org/abs/1709.01305\n",
      "====================================================\n",
      "Hierarchical Metric Learning for Optical Remote Sensing Scene Categorization (Akashdeep Goel - 1 August, 2018)\n",
      "We employ an iterative max-margin clustering strategy to obtain the hierarchical organization of the classes. Experiment results obtained on the large-scale NWPU-RESISC45 and the popular UC-Merced datasets demonstrate the efficacy of the proposed hierarchical metric learning based RS scene recognition strategy in comparison to the standard approaches.\n",
      "Link: https://arxiv.org/abs/1708.01494\n",
      "====================================================\n",
      "Brain Responses During Robot-Error Observation (Dominik Welke - 16 August, 2017)\n",
      "Our findings show that it was possible to decode both correctness and robot type for the majority of participants significantly, although often just slightly, above chance level. Our findings suggest that non-invasive recordings of brain responses elicited when observing robots indeed contain decodable information about the correctness of the robot's action and the type of observed robot.\n",
      "Link: https://arxiv.org/abs/1708.01465\n",
      "====================================================\n",
      "Visually Aligned Word Embeddings for Improving Zero-shot Learning (Ruizhi Qiao - 17 July, 2017)\n",
      "This strategy allows the learned VAWE to generalize to various ZSL methods and visual features. As evaluated via four state-of-the-art ZSL methods on four benchmark datasets, the VAWE exhibit consistent performance improvement.\n",
      "Link: https://arxiv.org/abs/1707.05427\n",
      "====================================================\n",
      "Learning the Latent \"Look\": Unsupervised Discovery of a Style-Coherent Embedding from Fashion Images (Wei-Lin Hsiao - 3 August, 2017)\n",
      "Our approach can organize galleries of outfits by style without requiring any style labels. Experiments on over 100K images demonstrate its promise for retrieving, mixing, and summarizing fashion images by their style.\n",
      "Link: https://arxiv.org/abs/1707.03376\n",
      "====================================================\n",
      "Do Deep Neural Networks Suffer from Crowding? (Anna Volokitin - 26 June, 2017)\n",
      "We find that visual similarity between the target and flankers also plays a role and that pooling in early layers of the network leads to more crowding. Additionally, we show that incorporating the flankers into the images of the training set does not improve performance with crowding.\n",
      "Link: https://arxiv.org/abs/1706.08616\n",
      "====================================================\n",
      "Deep Mixture of Diverse Experts for Large-Scale Visual Recognition (Tianyi Zhao - 23 June, 2017)\n",
      "Finally, all these base deep CNNs with diverse outputs (task spaces) are seamlessly combined to form a deep mixture of diverse experts for recognizing tens of thousands of atomic object classes. Our experimental results have demonstrated that our deep mixture of diverse experts algorithm can achieve very competitive results on large-scale visual recognition.\n",
      "Link: https://arxiv.org/abs/1706.07901\n",
      "====================================================\n",
      "Cross-Domain Perceptual Reward Functions (Ashley D. Edwards - 25 July, 2017)\n",
      "We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned rewards that represent the visual similarity between an agents state and a cross-domain goal image. We report results for learning the CDPRs with a deep neural network and using them to solve two tasks with deep reinforcement learning.\n",
      "Link: https://arxiv.org/abs/1705.09045\n",
      "====================================================\n",
      "Attributes2Classname: A discriminative model for attribute-based unsupervised zero-shot learning (Berkan Demirel - 5 August, 2017)\n",
      "In addition, our proposed approach renders text-only training possible, hence, the training can be augmented without the need to collect additional image data. The experimental results show that our method yields state-of-the-art results for unsupervised ZSL in three benchmark datasets.\n",
      "Link: https://arxiv.org/abs/1705.01734\n",
      "====================================================\n",
      "Deep Unsupervised Similarity Learning using Partially Ordered Sets (Miguel A Bautista - 11 April, 2017)\n",
      "The similarity learning and grouping procedure are integrated in a single model and optimized jointly. The proposed unsupervised approach shows competitive performance on detailed pose estimation and object classification.\n",
      "Link: https://arxiv.org/abs/1704.02268\n",
      "====================================================\n",
      "Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce (Devashish Shankar - 7 March, 2017)\n",
      "We then share the design decisions and trade-offs made while deploying the model to power Visual Recommendations across a catalog of 50M products, supporting 2K queries a second at Flipkart, India's largest e-commerce company. The deployment of our solution has yielded a significant business impact, as measured by the conversion-rate.\n",
      "Link: https://arxiv.org/abs/1703.02344\n",
      "====================================================\n",
      "Pilot system development in metre-scale laboratory discharge (Pavlo Kochkin - 12 January, 2017)\n",
      "The visual similarities between high-altitude sprites and laboratory pilots are striking and may indicate that they are two manifestations of the same natural phenomenon.\n",
      "Link: https://arxiv.org/abs/1701.03300\n",
      "====================================================\n",
      "Contextual Visual Similarity (Xiaofang Wang - 8 December, 2016)\n",
      "The learned feature weights encode the contextualized visual similarity specified by the user and can be used for attribute specific image search. We also show the usefulness of our contextualized similarity weighting scheme for different tasks, such as answering visual analogy questions and unsupervised attribute discovery.\n",
      "Link: https://arxiv.org/abs/1612.02534\n",
      "====================================================\n",
      "Fast On-Line Kernel Density Estimation for Active Object Localization (Anthony D. Rhodes - 16 November, 2016)\n",
      "In our system, prior situation knowledge is captured by a set of flexible, kernel-based density estimations---a situation model---that represent the expected spatial structure of the given situation. These estimations are efficiently updated by information gained as the system searches for relevant objects, allowing the system to use context as it is discovered to narrow the search.\n",
      "Link: https://arxiv.org/abs/1611.05369\n",
      "====================================================\n",
      "Indoor Space Recognition using Deep Convolutional Neural Network: A Case Study at MIT Campus (Fan Zhang - 7 October, 2016)\n",
      "Second, we demonstrate that DCNN also has a potential capability in space feature learning and recognition, even under severe appearance changes. Third, we introduce a DCNN based approach to look into the visual similarity and visual distinctiveness of interior space.\n",
      "Link: https://arxiv.org/abs/1610.02414\n",
      "====================================================\n",
      "Visual Fashion-Product Search at SK Planet (Taewan Kim - 11 April, 2017)\n",
      "In addition to these fashion-attributes for semantic similarity, we extract colour and appearance features in a region-of-interest (ROI) of a fashion item for visual similarity. By sharing our approach, we expect active discussion on that how to apply current computer vision research into the e-commerce industry.\n",
      "Link: https://arxiv.org/abs/1609.07859\n",
      "====================================================\n",
      "Automatic Visual Theme Discovery from Joint Image and Text Corpora (Ke Sun - 7 September, 2016)\n",
      "In experiments, visual themes significantly outperforms tags on semantic image understand- ing and achieve state-of-art performance in all three tasks. This again demonstrate the effectiveness and versatility of proposed framework.\n",
      "Link: https://arxiv.org/abs/1609.01859\n",
      "====================================================\n",
      "CliqueCNN: Deep Unsupervised Exemplar Learning (Miguel A. Bautista - 31 August, 2016)\n",
      "The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.\n",
      "Link: https://arxiv.org/abs/1608.08792\n",
      "====================================================\n",
      "Automatic Synchronization of Multi-User Photo Galleries (E. Sansone - 16 January, 2017)\n",
      "The experimental evaluation is conducted on four publicly available datasets covering different types of events, demonstrating the strength of our proposed method. A thorough discussion of the obtained results is provided for a critical assessment of the quality in synchronization.\n",
      "Link: https://arxiv.org/abs/1608.06770\n",
      "====================================================\n",
      "Active Object Localization in Visual Situations (Max H. Quinn - 2 July, 2016)\n",
      "We compare the results with several baselines and variations on our method, and demonstrate the strong benefit of using situation knowledge and active context-driven localization. Finally, we contrast our method with several other approaches that use context as well as active search for object localization in images.\n",
      "Link: https://arxiv.org/abs/1607.00548\n",
      "====================================================\n",
      "Human Centred Object Co-Segmentation (Chenxia Wu - 12 June, 2016)\n",
      "Moreover, the auto-encoder learns the parameters from the data itself rather than supervised learning or manually assigned parameters in the conventional CRF. In the extensive experiments on four datasets, we show that our approach is able to extract the common objects more accurately than the state-of-the-art co-segmentation algorithms.\n",
      "Link: https://arxiv.org/abs/1606.03774\n",
      "====================================================\n",
      "Summary Transfer: Exemplar-based Subset Selection for Video Summarization (Ke Zhang - 29 April, 2016)\n",
      "We also show how to generalize our method to subshot-based summarization, which not only reduces computational costs but also provides more flexible ways of defining visual similarity across subshots spanning several frames. We conduct extensive evaluation on several benchmarks and demonstrate promising results, outperforming existing methods in several settings.\n",
      "Link: https://arxiv.org/abs/1603.03369\n",
      "====================================================\n",
      "Learning Concept Embeddings with Combined Human-Machine Expertise (Michael J. Wilber - 28 September, 2015)\n",
      "We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods.\n",
      "Link: https://arxiv.org/abs/1509.07479\n",
      "====================================================\n",
      "Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences (Andreas Veit - 24 September, 2015)\n",
      "While this approach is applicable to a wide variety of settings, we focus on the representative problem of learning compatible clothing style. Our results indicate that the proposed framework is capable of learning semantic information about visual style and is able to generate outfits of clothes, with items from different categories, that go well together.\n",
      "Link: https://arxiv.org/abs/1509.07473\n",
      "====================================================\n",
      "Dual-Layer Video Encryption using RSA Algorithm (Aman Chadha - 14 September, 2015)\n",
      "For applications wherein visual similarity is not of major concern, we limit the encryption task to a single level of encryption which is accomplished by using RSA, thereby quickening the encryption process. Although some similarity between the original and encrypted video is observed in this case, it is not enough to comprehend the happenings in the video.\n",
      "Link: https://arxiv.org/abs/1509.04387\n",
      "====================================================\n",
      "Galaxy morphology - an unsupervised machine learning approach (Andrew Schutter - 23 May, 2015)\n",
      "Rather than relying on human cognition, the proposed system deduces the similarities between sets of galaxy images in an automatic manner, and is therefore not limited by the number of galaxies being analyzed. The source code of the method is publicly available, and the protocol of the experiment is included in the paper so that the experiment can be replicated, and the method can be used to analyze user-defined datasets of galaxy images.\n",
      "Link: https://arxiv.org/abs/1505.04876\n",
      "====================================================\n",
      "Formation of shear-bands in drying colloidal dispersions (Pree-Cha Kiatkirakajorn - 24 September, 2015)\n",
      "We further show how the bands form, that they scale with the thickness of the drying layer, and that they are eliminated by the addition of salt to the drying dispersions. Finally, we reveal the origins of these bands in the compressive forces associated with drying, and show how they affect the optical properties (birefringence) of colloidal films and coatings.\n",
      "Link: https://arxiv.org/abs/1505.02515\n",
      "====================================================\n",
      "Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature (Babak Saleh - 4 May, 2015)\n",
      "We develop a machine that is able to make aesthetic-related semantic-level judgments, such as predicting a painting's style, genre, and artist, as well as providing similarity measures optimized based on the knowledge available in the domain of art historical interpretation. Our experiments show the value of using this similarity measure for the aforementioned prediction tasks.\n",
      "Link: https://arxiv.org/abs/1505.00855\n",
      "====================================================\n",
      "A Data-Driven Approach for Tag Refinement and Localization in Web Videos (Lamberto Ballan - 28 May, 2015)\n",
      "Compared to existing video tagging approaches that require training classifiers for each tag, our system has few parameters, is easy to implement and can deal with an open vocabulary scenario. We demonstrate the approach on tag refinement and localization on DUT-WEBV, a large dataset of web videos, and show state-of-the-art results.\n",
      "Link: https://arxiv.org/abs/1407.0623\n",
      "====================================================\n",
      "A Visual Grammar Approach for TV Program Identification (Tarek Zlitni - 10 January, 2013)\n",
      "The main idea of identification process consists in comparing the visual similarity of the video signal signature in TV stream to the catalogue elements. After presenting the proposed approach, the paper overviews the encouraging experimental results on several streams extracted from different channels and composed of several programs.\n",
      "Link: https://arxiv.org/abs/1301.2200\n",
      "====================================================\n",
      "Propagating mode-I fracture in amorphous materials using the continuous random network (CRN) model (Shay I Heizler - 10 January, 2011)\n",
      "Beside the qualitative visual similarity of the simulated cracks to experiment, the simulation also succeeds in explaining the experimentally observed oscillations of the crack velocity.\n",
      "Link: https://arxiv.org/abs/1101.1432\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
